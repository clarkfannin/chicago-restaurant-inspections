name: daily_data_pipeline

permissions: 
  id-token: write
  contents: read

on:
  schedule:
    - cron: "0 3 * * *" # runs daily at 3 AM UTC
  workflow_dispatch:

jobs:
  run-pipeline:
    runs-on: ubuntu-latest

    env:
      SUPABASE_DB_URL: ${{ secrets.SUPABASE_DB_URL }}
      CHICAGO_API_TOKEN: ${{ secrets.CHICAGO_API_TOKEN }}
      GOOGLE_API_KEY: ${{ secrets.GOOGLE_API_KEY }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v3
        with:
          role-to-assume: arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/inspection-oidc
          aws-region: us-east-2

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: pip install -r requirements.txt

      - name: Fetch and load Chicago data
        run: python data/load_data.py
        env:
          DATABASE_URL: ${{ secrets.SUPABASE_DB_URL }}
          CHICAGO_API_TOKEN: ${{ secrets.CHICAGO_API_TOKEN }}

      - name: Dump Supabase tables to CSV
        run: |
          mkdir -p dumps
          echo "Cleaning up old CSVs..."
          rm -f dumps/*.csv
          for table in restaurants inspections google_ratings; do
            echo "Dumping $table..."
            psql "$SUPABASE_DB_URL" -c "\copy $table TO 'dumps/$table.csv' CSV HEADER"
          done

      - name: Upload CSVs to S3
        run: |
          for csv_file in dumps/*.csv; do
            echo "Uploading $csv_file to S3..."
            aws s3 cp "$csv_file" s3://inspection-dump/
          done
